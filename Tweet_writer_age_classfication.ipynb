{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0O1SibhiXRGu",
        "zPrLpzK7XMGz",
        "ykLXMXrFXdZW",
        "pEbJZb88Xgpx",
        "L11S8dH2Xj-8",
        "zDEfyoeOXpkf",
        "-QQmKuyU7m9P",
        "_Sc0xtns8UQC",
        "lc00an28E4es"
      ],
      "authorship_tag": "ABX9TyMNMpprxO9OwpM8IOm37KGR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deema1999/Tweets_writers_age_classification/blob/main/Tweet_writer_age_classfication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_NCxvYM2MlI"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import xml.etree.cElementTree as et\n",
        "import re\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXAkVSp-_1zJ"
      },
      "source": [
        "# copy images\n",
        "!mkdir -p Dataset\n",
        "!cp -rf /content/gdrive/MyDrive/ML/pan2015 Dataset\n",
        "!cp -rf /content/gdrive/MyDrive/ML/truth.txt Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcx2j1IXknCV"
      },
      "source": [
        "#!unzip /content/Project_Data.zip -d Project_Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5soGoY1SCC6w"
      },
      "source": [
        "# Play with data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "data = \"His sellection is bery antresting\"\n",
        "output = TextBlob(data).correct()\n",
        "output"
      ],
      "metadata": {
        "id": "jVa41uVR5pBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fieNOjD6_2sY"
      },
      "source": [
        "documents = {}\n",
        "ind = -1\n",
        "from textblob import TextBlob\n",
        "\n",
        "for filename in os.listdir('/content/Dataset/pan2015'):\n",
        "    full_path = os.path.join('/content/Dataset/pan2015', filename)\n",
        "    tree = et.parse(full_path)\n",
        "    root = tree.getroot()\n",
        "    id = root.attrib['id']\n",
        "    document = \"\" \n",
        "    ind+=1\n",
        "    for doc in root.iter('document'):  \n",
        "      text = doc.text\n",
        "      text = text.lower()\n",
        "      text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
        "      # Removing URL's\n",
        "      # text = re.sub(r'http\\S+', '', text)\n",
        "      # Removing puctuations\n",
        "      # text = re.sub(r'[^\\w\\s]', '', text)\n",
        "      # Removing numbers\n",
        "      # text = re.sub(r'[\\d-]', '', text)\n",
        "      # Removing multiple spaces\n",
        "      #text = re.sub(r'\\s\\s+', '', text)\n",
        "      # Removing redundent charachters\n",
        "      #text = re.sub(r'([a-z]+?)\\1+', r'\\1',text)\n",
        "      document = document + text + ''\n",
        "    documents[ind] = [id,document]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwpCcfDy73sV"
      },
      "source": [
        "df = pd.DataFrame.from_dict(documents , orient='index' , columns=['Id','Tweets'])\n",
        "df[df['Id'] == \"703806e5-f04b-4e7c-8456-7340784ecf76\"]['Tweets']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8upa02GBV0E"
      },
      "source": [
        "#input file\n",
        "fin = open(\"/content/Dataset/truth.txt\", \"rt\")\n",
        "#output file to write the result to\n",
        "fout = open(\"out.txt\", \"wt\")\n",
        "#for each line in the input file\n",
        "for line in fin:\n",
        "\t#read replace the string and write to output file\n",
        "\tfout.write(line.replace(':::', ','))\n",
        "#close input and output files\n",
        "fin.close()\n",
        "fout.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWBSoU5dsSAU"
      },
      "source": [
        "\n",
        "def prepend_line(file_name, line):\n",
        "    \"\"\" Insert given string as a new line at the beginning of a file \"\"\"\n",
        "    # define name of temporary dummy file\n",
        "    dummy_file = file_name + '.txt'\n",
        "    # open original file in read mode and dummy file in write mode\n",
        "    with open(file_name, 'r') as read_obj, open(dummy_file, 'w') as write_obj:\n",
        "        # Write given line to the dummy file\n",
        "        write_obj.write(line + '\\n')\n",
        "        # Read lines from original file one by one and append them to the dummy file\n",
        "        for line in read_obj:\n",
        "            write_obj.write(line)\n",
        "    # remove original file\n",
        "    os.remove(file_name)\n",
        "    # Rename dummy file as the original file\n",
        "    os.rename(dummy_file, file_name)\n",
        "\n",
        "prepend_line(\"out.txt\", \"Id,Sex,Age,V,X,Y,Z,W\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZf1yWKWto1U"
      },
      "source": [
        "truth = pd.read_csv(\"out.txt\")\n",
        "# storing this dataframe in a csv file\n",
        "truth.to_csv('out.csv', index = None)\n",
        "truth.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQMEjK5jufFf"
      },
      "source": [
        "data = pd.merge(df, truth, on=\"Id\")\n",
        "data.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TklTd0lzwz_P"
      },
      "source": [
        "data['Age'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install demoji"
      ],
      "metadata": {
        "id": "aL3fGXFy32ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eIc0X7RyjhP"
      },
      "source": [
        "import demoji\n",
        "data_copy = data.copy()\n",
        "target_columns = ['Tweets','Age']\n",
        "data_copy = data_copy[target_columns]\n",
        "data_copy.head()\n",
        "t = data_copy['Tweets'][5]\n",
        "demoji.findall(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXuYyAwrzBRh"
      },
      "source": [
        "# Explore data\n",
        "print(data_copy['Age'].unique())\n",
        "print(\"All types : \",list(set(data_copy.dtypes.tolist())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoOZCrO6zbGu"
      },
      "source": [
        "# To check missing values\n",
        "print('Sum : \\n',data_copy.isnull().sum())\n",
        "sns.heatmap(data_copy.isnull(), cbar=False, yticklabels=False, cmap=\"viridis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFhEI7dZz3qD"
      },
      "source": [
        "# Dropping the duplicate rows\n",
        "duplicate_rows = data_copy[data_copy.duplicated()]\n",
        "print(\"number of duplicate rows : \",duplicate_rows.shape)\n",
        "\n",
        "data_copy.drop_duplicates(inplace=True)\n",
        "print(\"number of duplicate rows : \",data_copy[data_copy.duplicated()].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xi3BTWMx0X7t"
      },
      "source": [
        "# data visualization\n",
        "sns.countplot(data_copy['Age'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9eyNmwv3bdB"
      },
      "source": [
        "data_copy[\"Age\"] = data_copy[\"Age\"].map({\"18-24\":1, \"25-34\":int(2), \"35-49\":int(3), \"50-XX\":int(4)})\n",
        "print(data_copy.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjsea5CG4Djl"
      },
      "source": [
        "# Models Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhBKB6o24G58"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "Y = data_copy[\"Age\"].values\n",
        "# split the dataset into training data and test data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_NB, X_test_NB, Y_train_NB, Y_test_NB = train_test_split(data_copy, Y, \n",
        "                        test_size= 0.20, random_state=100, stratify=Y) # 0.35 ==> SVM : 76\n",
        "\n",
        "vec = CountVectorizer()\n",
        "train_vectors = vec.fit_transform(X_train_NB['Tweets'])\n",
        "test_vectors = vec.transform(X_test_NB['Tweets'])\n",
        "\n",
        "print(\"train data : \", train_vectors.shape)\n",
        "print(\"test data : \", test_vectors.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O1SibhiXRGu"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPVFtiWq4O_A"
      },
      "source": [
        "# Classify using Naive Bayes \n",
        "# with tfidf : 0.58 , with countVec 0.71 || First method when combining user's tweets\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "MNB_clf = MultinomialNB(alpha=0.5)\n",
        "\n",
        "# fit model no training data\n",
        "MNB_clf.fit(train_vectors, Y_train_NB)\n",
        "\n",
        "# making predictions on the testing set \n",
        "predicted = MNB_clf.predict(test_vectors)\n",
        "\n",
        "# Classification report\n",
        "from sklearn import metrics\n",
        "print(\"Classification report : \\n\", MNB_clf,\"\\n\", \n",
        "      metrics.classification_report(Y_test_NB, predicted))\n",
        "disp = metrics.plot_confusion_matrix(MNB_clf, test_vectors, Y_test_NB)\n",
        "disp.figure_.suptitle(\"Confusion Matrix\")\n",
        "print(\"Confusion matrix: \\n\", disp.confusion_matrix)\n",
        "\n",
        "print(\"--------------------------------------\")\n",
        "accuracies = cross_val_score(estimator = MNB_clf, X = train_vectors, y = Y_train_NB, cv = 10)\n",
        "print(\"\\nAccuracy with 10-kfold: {:.2f} %\".format(accuracies.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPrLpzK7XMGz"
      },
      "source": [
        "# Logistic Reg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81B1IOVW5D2c"
      },
      "source": [
        "# Classify using Logistic Regression #0.71\n",
        "# with tfidf : 0.71 , with countVec 0.84\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# all parameters not specified are set to their defaults\n",
        "logisticRegr = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=10, random_state=42)\n",
        "\n",
        "# fit model no training data\n",
        "logisticRegr.fit(train_vectors, Y_train_NB)\n",
        "\n",
        "# making predictions on the testing set \n",
        "predicted = logisticRegr.predict(test_vectors)\n",
        "\n",
        "# Classification report\n",
        "from sklearn import metrics\n",
        "print(\"Classification report : \\n\", logisticRegr,\"\\n\", \n",
        "      metrics.classification_report(Y_test_NB, predicted))\n",
        "disp = metrics.plot_confusion_matrix(logisticRegr, test_vectors, Y_test_NB)\n",
        "disp.figure_.suptitle(\"Confusion Matrix\")\n",
        "print(\"Confusion matrix: \\n\", disp.confusion_matrix)\n",
        "\n",
        "print(\"------------------------------------------------\")\n",
        "accuracies = cross_val_score(estimator = logisticRegr, X = train_vectors, y = Y_train_NB, cv = 10)\n",
        "print(\"\\nAccuracy with 10-kfold: {:.2f} %\".format(accuracies.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8AK7IlIXX29"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdOEbHxO5HeU"
      },
      "source": [
        "# Classify using SVM - 0.77 after tuning 0.81 by using C=10\n",
        "# with tfidf : 0.81 , with countVec 0.81\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "import pickle\n",
        "# all parameters not specified are set to their defaults\n",
        "svmClassifier = SVC(kernel=\"rbf\", C=10, gamma=0.0001)\n",
        "#svm_classifier = SVC()\n",
        "\n",
        "\n",
        "#svm_classifier = SVC(kernel=\"poly\", degree=5, C=10, gamma=0.01)\n",
        "#svm_classifier = SVC(kernel=\"sigmoid\", C=10, gamma=0.001)\n",
        "#svm_classifier = SVC(kernel=\"rbf\", C=300, gamma=0.001)\n",
        "\n",
        "# fit model no training data\n",
        "svmClassifier.fit(train_vectors, Y_train_NB)\n",
        "\n",
        "# making predictions on the testing set \n",
        "predicted = svmClassifier.predict(test_vectors)\n",
        "\n",
        "# Classification report\n",
        "from sklearn import metrics\n",
        "print(\"Classification report : \\n\", svmClassifier,\"\\n\", \n",
        "      metrics.classification_report(Y_test_NB, predicted))\n",
        "disp = metrics.plot_confusion_matrix(svmClassifier, test_vectors, Y_test_NB)\n",
        "disp.figure_.suptitle(\"Confusion Matrix\")\n",
        "print(\"Confusion matrix: \\n\", disp.confusion_matrix)\n",
        "\n",
        "'''\n",
        "print(\"------------------------------------------------\")\n",
        "accuracies = cross_val_score(estimator = svmClassifier, X = train_vectors, y = Y_train_NB, cv = 10)\n",
        "print(\"\\nAccuracy with 10-kfold: {:.2f} %\".format(accuracies.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))'''\n",
        "pickle.dump(svmClassifier, open(\"svc_model\", 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-sZk1diJEpE"
      },
      "source": [
        "loaded_model = pickle.load(open(\"svc_model\", 'rb'))\n",
        "result = loaded_model.score(test_vectors, Y_test_NB)\n",
        "print(result)\n",
        "\n",
        "#Classification report\n",
        "from sklearn import metrics\n",
        "disp = metrics.plot_confusion_matrix(svmClassifier, test_vectors, Y_test_NB)\n",
        "disp.figure_.suptitle(\"Confusion Matrix\")\n",
        "print(\"Confusion matrix: \\n\", disp.confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykLXMXrFXdZW"
      },
      "source": [
        "# KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH4mGZKu5Lsh"
      },
      "source": [
        "# Classify using KNN 0.65\n",
        "# with tfidf : 0.65 , with countVec 0.52\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "\n",
        "# Create the classifier\n",
        "\"\"\"\n",
        "algorithm{‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, default=’auto’\n",
        "Algorithm used to compute the nearest neighbors:\n",
        "\n",
        "‘ball_tree’ will use BallTree\n",
        "\n",
        "‘kd_tree’ will use KDTree\n",
        "\n",
        "‘brute’ will use a brute-force search.\n",
        "\n",
        "‘auto’ will attempt to decide the most appropriate algorithm based on the \n",
        "values passed to fit method.\n",
        "\"\"\"\n",
        "knn_cls = KNeighborsClassifier(n_neighbors = 10) # 77%\n",
        "\n",
        "# Train the classifier\n",
        "knn_cls.fit(train_vectors,Y_train_NB)\n",
        "\n",
        "# Predict the value of X_test\n",
        "predicted = knn_cls.predict(test_vectors)\n",
        "\n",
        "# Classification report\n",
        "from sklearn import metrics\n",
        "print(\"Classifier : \", knn_cls)\n",
        "print(\"Classification report for classifier : \\n\",\n",
        "      metrics.classification_report(Y_test_NB, predicted))\n",
        "\n",
        "disp = metrics.plot_confusion_matrix(knn_cls, test_vectors, Y_test_NB)\n",
        "disp.figure_.suptitle(\"Confusion Matrix\")\n",
        "print(\"Confusion matrix: \\n\", disp.confusion_matrix)\n",
        "\n",
        "'''\n",
        "print(\"-------------------------------------------\")\n",
        "accuracies = cross_val_score(estimator = knn_cls, X = train_vectors, y = Y_train_NB, cv = 10)\n",
        "print(\"\\nAccuracy with 10-kfold: {:.2f} %\".format(accuracies.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEbJZb88Xgpx"
      },
      "source": [
        "# SGD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw9sYLSC5at0"
      },
      "source": [
        "# Classify using SGD 0.81\n",
        "# with tfidf : 0.81 , with countVec 0.84\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "# all parameters not specified are set to their defaults\n",
        "sgdClassifier = SGDClassifier(random_state=42)\n",
        "\n",
        "# fit model no training data\n",
        "sgdClassifier.fit(train_vectors, Y_train_NB)\n",
        "\n",
        "# making predictions on the testing set \n",
        "predicted = sgdClassifier.predict(test_vectors)\n",
        "\n",
        "# Classification report\n",
        "from sklearn import metrics\n",
        "print(\"Classification report : \\n\", sgdClassifier,\"\\n\", \n",
        "      metrics.classification_report(Y_test_NB, predicted))\n",
        "disp = metrics.plot_confusion_matrix(sgdClassifier, test_vectors, Y_test_NB)\n",
        "disp.figure_.suptitle(\"Confusion Matrix\")\n",
        "print(\"Confusion matrix: \\n\", disp.confusion_matrix)\n",
        "\n",
        "print(\"-------------------------------------------\")\n",
        "accuracies = cross_val_score(estimator = sgdClassifier, X = train_vectors, y = Y_train_NB, cv = 10)\n",
        "print(\"\\nAccuracy with 10-kfold: {:.2f} %\".format(accuracies.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L11S8dH2Xj-8"
      },
      "source": [
        "# XGBoosting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdHy0xKy5fVE"
      },
      "source": [
        "# Classify using XGBoosting 0.65\n",
        "# with tfidf : 0.65 , with countVec 0.71\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from xgboost import XGBClassifier\n",
        "XG_clf = XGBClassifier(random_state=100)\n",
        "\n",
        "# fit model no training data\n",
        "XG_clf.fit(train_vectors, Y_train_NB)\n",
        "\n",
        "# make predictions for test data\n",
        "y_pred = XG_clf.predict(test_vectors)\n",
        "predicted = [round(value) for value in y_pred]\n",
        "\n",
        "# Classification report\n",
        "from sklearn import metrics\n",
        "print(\"Classification report : \\n\", XG_clf,\"\\n\", \n",
        "      metrics.classification_report(Y_test_NB, predicted))\n",
        "disp = metrics.plot_confusion_matrix(XG_clf, test_vectors, Y_test_NB)\n",
        "disp.figure_.suptitle(\"Confusion Matrix\")\n",
        "print(\"Confusion matrix: \\n\", disp.confusion_matrix)\n",
        "\n",
        "print(\"------------------------------------------------\")\n",
        "accuracies = cross_val_score(estimator = svmClassifier, X = train_vectors, y = Y_train_NB, cv = 10)\n",
        "print(\"\\nAccuracy with 10-kfold: {:.2f} %\".format(accuracies.mean()*100))\n",
        "print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOTTFmolXmuW"
      },
      "source": [
        "# MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQr3bt3t5lgG"
      },
      "source": [
        "# Classify using MLPClassifier 0.81(50/100,50)\n",
        "# with tfidf : 0.81 , with countVec 0.84(100,50) 0.87(100,50,50)\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100,50, ), batch_size=64,\n",
        "                    solver=\"adam\", alpha=0.005, activation=\"logistic\", \n",
        "                    max_iter=50000, random_state=42)\n",
        "\n",
        "\n",
        "# fit model no training data\n",
        "mlp.fit(train_vectors, Y_train_NB)\n",
        "\n",
        "# make predictions for test data\n",
        "predicted = mlp.predict(test_vectors)\n",
        "\n",
        "# Classification report\n",
        "from sklearn import metrics\n",
        "print(\"Classification report : \\n\", mlp,\"\\n\", \n",
        "      metrics.classification_report(Y_test_NB, predicted))\n",
        "disp = metrics.plot_confusion_matrix(mlp, test_vectors, Y_test_NB)\n",
        "disp.figure_.suptitle(\"Confusion Matrix\")\n",
        "print(\"Confusion matrix: \\n\", disp.confusion_matrix)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDEfyoeOXpkf"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1cpU9rEO-_0"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "# training the model on training set\n",
        "\n",
        "# n_estimators : The number of trees in the forest.\n",
        "# max_depth : The maximum depth of the tree.\n",
        "# n_jobsint : The number of jobs to run in parallel\n",
        "rf_clf = RandomForestClassifier(n_estimators=50, max_depth=10)\n",
        "rf_clf.fit(train_vectors,Y_train_NB)\n",
        "\n",
        "# making predictions on the testing set \n",
        "predicted = rf_clf.predict(test_vectors)\n",
        "\n",
        "# comparing actual response values (y_test) with predicted response values (predicted)\n",
        "print(\"Classification report : \\n\", rf_clf,\"\\n\", metrics.classification_report(Y_test_NB, predicted))\n",
        "disp = metrics.plot_confusion_matrix(rf_clf, test_vectors, Y_test_NB)\n",
        "disp.figure_.suptitle(\"Confusion Matrix\")\n",
        "print(\"Confusion matrix: \\n\", disp.confusion_matrix)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QQmKuyU7m9P"
      },
      "source": [
        "# Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWpC4tAn7pbG"
      },
      "source": [
        "# Cross-validation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "Y = data_copy[\"Age\"].values\n",
        "vec = CountVectorizer()\n",
        "\n",
        "# split the dataset into training data and test data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train_NB, X_test_NB, Y_train_NB, Y_test_NB = train_test_split(data_copy, Y, \n",
        "                        test_size= 0.20, random_state=100, stratify=Y)\n",
        "\n",
        "\n",
        "train_vectors = vec.fit_transform(X_train_NB['Tweets'])\n",
        "test_vectors = vec.transform(X_test_NB['Tweets'])\n",
        "\n",
        "print(\"\\n K-fold Cross-Validation : *********\")\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=10, random_state=100)\n",
        "model_2 = SVC(C= 10, gamma=0.0001, kernel='rbf')\n",
        "results_model_2 = cross_val_score(model_2, train_vectors, Y_train_NB, cv=kfold)\n",
        "print(\"Scores   2 :\",results_model_2)\n",
        "print(\"Accuracy 2 :\",results_model_2.mean())\n",
        "print(\"std      2 :\",np.std(results_model_2))\n",
        "\n",
        "'''\n",
        "print(\"\\n Stratified K-fold Cross-Validation : *********\")\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "skfold = StratifiedKFold(n_splits=10, random_state=100)\n",
        "model_3 = SVC(kernel=\"linear\", C=10.0, gamma= 0.1)\n",
        "results_model_3 = cross_val_score(model_3, train_vectors, Y_train_NB[:7000], cv=skfold)\n",
        "print(\"Scores   3 :\",results_model_3)\n",
        "print(\"Accuracy 3 :\",results_model_3.mean())\n",
        "print(\"std      3 :\",np.std(results_model_3))'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Sc0xtns8UQC"
      },
      "source": [
        "# Hyperparameters Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFrXJH7R8YVP"
      },
      "source": [
        "# Hyperparameters Tuning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "param_grid = [\n",
        "  {\"C\": [1, 10, 50, 100, 500, 1000], \"kernel\": [\"linear\"]},\n",
        "  {\"C\": [1, 10, 100, 1000],\"degree\":[3, 5, 7],  \"gamma\": [0.001, 0.0001], \"kernel\": [\"poly\"]},\n",
        "  {\"C\": [1, 10, 20, 50, 100, 500, 1000, 2000], \"gamma\": [0.001, 0.0001 , 0.0003], \"kernel\": [\"rbf\", \"sigmoid\"]},\n",
        "  # {'alpha':[0.0001 ,0.5, 0.001 , 2 , 5 ,20]}\n",
        "  # {\"hidden_layer_sizes\":[(100,50, ) ,(100,100)], \"batch_size\":[64],\"solver\":[\"adam\"], \"alpha\":[0.005 , 0.0001], \"activation\":[\"logistic\"], \"max_iter\":[50000], \"random_state\":[42]}\n",
        "]\n",
        "\n",
        "grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 1) \n",
        "grid.fit(train_vectors, Y_train_NB)\n",
        "\n",
        "print(\"----------------------------------------------------------------------\")\n",
        "print(\"The best parameter after tuning :\",grid.best_params_)  \n",
        "print(\"Our model looks after hyper-parameter tuning\",grid.best_estimator_)\n",
        "\n",
        "predicted = grid.predict(test_vectors)\n",
        "\n",
        "from sklearn import metrics\n",
        "print(\"Classification report : \\n\", grid,\"\\n\", \n",
        "      metrics.classification_report(Y_test_NB, predicted))\n",
        "disp = metrics.plot_confusion_matrix(grid, test_vectors, Y_test_NB)\n",
        "disp.figure_.suptitle(\"Confusion Matrix\")\n",
        "print(\"Confusion matrix: \\n\", disp.confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc00an28E4es"
      },
      "source": [
        "# Gradio"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qZHyiW-E6P-"
      },
      "source": [
        "!pip install gradio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnkmjGZVE8Jv"
      },
      "source": [
        "# https://www.gradio.app/ml_examples\n",
        "# https://www.kaggle.com/scolianni/mnistasjpg\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "import requests\n",
        "\n",
        "def inference(tweet):\n",
        "    labels = ['18-24', '25-34', '35-49', '50-XX']\n",
        "    texts = vec.transform([tweet])\n",
        "    pred = mlp.predict_proba(texts)\n",
        "    #pred = np.append(pred,int(not pred[0]))\n",
        "    dictionary = dict(zip(labels, map(float, pred[0])))\n",
        "    return dictionary\n",
        "\n",
        "\n",
        "gr.Interface(fn=inference, \n",
        "             inputs=\"textbox\", \n",
        "             outputs=gr.outputs.Label(num_top_classes=4)).launch(share=True) #, debug=True Use in Colab\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpjmQBpbFfnU"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}